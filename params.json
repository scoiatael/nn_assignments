{"name":"NeuralNetworks - CIFAR10","tagline":"Code for Neural Networks 2015 class, w/ neural network for solving CIFAR10 project.","body":"## CIFAR10 Solution\r\n\r\nThis solution achieves about 75% accuracy on CIFAR10 dataset.\r\n\r\n### Architecture\r\nArchitecture is based on, by many considered canonical, [LeNet](http://yann.lecun.com/exdb/lenet/). It consists of layers:\r\n * NL - linear + ReLU nonlinearity,\r\n * C2 - 2d convolution,\r\n * M2 - 2d max pooling,\r\n * DB - dropout of 70%,\r\n * DS - dropout of 30%,\r\n * SM - softmax layer.\r\nThey are structured in following way:\r\n\r\nC2 -> M2 -> C2 -> M2 -> C2 -> DS -> NL -> DB -> NL -> SM.\r\n\r\nThis way convolution layers should act as feature extractors, nonlinear layers should act as multilayer perceptron, and softmax layer should produce desired results. Dropout layers were added as a means of reducing chance of overfitting.\r\n\r\n### Training\r\nNetwork was trained using Nesterov momentum (improved version of Stochastic Gradient Descent).\r\n\r\nNetwork was trained on examples from normal and inverted (features scaled by -1) train stream. Each training pass consisted of doing 5 full passes over inverted train set, then 10 over normal. Each full pass updated network.\r\n\r\nMotivation of using inverted features was to force net to look at shape, not strictly colour of things. To make up for extra noise generated by these examples, they were assigned much lower learning rate (0.1 of the original) and had less passes.To fully assess that approach, more data should be tested and generalized, although here it showed some promise: net trained much faster and with better overall results in this manner. But this may be simply because those examples generated extra noise.\r\n\r\nMajority (4/5) of train examples were used for training, and rest for validation.\r\n\r\nWhole training took 25 training passes (which means that network was fed train set 500 times). With each train set pass taking between as few as 5 seconds (thanks to Theano CUDA/GPU support) up to 20 seconds, this was much shorter then expected time for CIFAR10 network.\r\n\r\n### Tools\r\nNetwork was written with [Lasagne](http://lasagne.readthedocs.org/en/latest/), a simple Python library for building and training neural networks in Theano. It provides user with ready-to-use variants of most popular layers and training functions. For the purposes of this network, it had everything in place.\r\n\r\n### Hyper parameters\r\nHyper parameters were chosen by trial-and-error, with running multiple versions of network and choosing those that performed best. Initial values were taken from example networks for MNIST from lasagne repository.\r\n\r\n* __Momentum__ equal to `0.2`. With initial value `0.9` network was initially learning very quickly, but also stopped learning pretty quickly. Decreasing momentum further slowed down initial progress, but increased overall accuracy.\r\n\r\n* __Learning rate__ equal to `0.01*0.1**(iteration_numer/100.0)`. While constant learning rate is often suggested, here it was found that decreasing learning rate with time offered better results.\r\n\r\n* Convolution layers used `32` __learnable convolutional filters__ and `5x5` __filter size__. It was found that increasing filter size actually decreased network accuracy, while increasing filter size made no difference for final accuracy but increased learning time.\r\n\r\n* Max pooling layers used `2x2` __pool size__. With 32x32 images, increasing that parameter was impossible.\r\n\r\n* Dropout layers used __dropout probability__ equal to `0.7` (for deeper layers) and `0.3` (for closer layers).","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}