{"name":"NeuralNetworks - CIFAR10","tagline":"Code for Neural Networks 2015 class, w/ neural network for solving CIFAR10 project.","body":"## CIFAR10 Solution\r\n\r\nThis solution achieves about 75% accuracy on CIFAR10 dataset.\r\n\r\n### Architecture\r\nArchitecture is based on, by many considered canonical, [LeNet](http://yann.lecun.com/exdb/lenet/), with added nonlinear layers. It consists of following layer types:\r\n * Nonlinear (big) - linear + ReLU nonlinearity with 800 units,\r\n * Nonlinear (small) - linear + ReLU nonlinearity with 256 units,\r\n * Convolution - 2d convolution with 32 filters and 5x5 filter size,\r\n * Maxpool - 2d max pooling with 2x2 pool sizes,\r\n * Drouput (big) - dropout of 50%,\r\n * Droupout (small) - dropout of 20%,\r\n * Softmax - softmax layer, casting to final 10 classes.\r\nThey are structured in following way:\r\n\r\n![C2 -> M2 -> C2 -> M2 -> C2 -> DS -> NL -> DB -> NL -> SM.](https://cloud.githubusercontent.com/assets/2892794/12703521/14605fd2-c845-11e5-83ee-214e00fcee79.png)\r\n\r\nThis way convolution layers should act as feature extractors, nonlinear layers should act as multilayer perceptron, and softmax layer should produce desired results. Dropout layers were added as a means of reducing chance of overfitting.\r\n\r\n### Training\r\nNetwork was trained using Nesterov momentum (improved version of Stochastic Gradient Descent).\r\n\r\nNetwork was trained on examples from normal and inverted (features scaled by -1) train stream. Each training pass consisted of doing 10 full passes over inverted train set, then 20 over normal. Each full pass updated network.\r\n\r\nThen there were 50 single passes over only normal train stream - they were meant to finalize network and allow it to converge to best result on normal data.\r\n\r\nMotivation for using inverted features was to force net to focus on shape, not colour of things. To make up for extra noise generated by these examples, they had less passes.To fully assess that approach, more data should be tested and results should be generalized. Here it showed some hope: net trained much faster and with better overall results in this manner. But this may be simply because those examples generated extra noise, which forced network out of local minima.\r\n\r\nMajority (4/5) of train examples were used for training, and rest for validation.\r\n\r\nWhole training took 58 iterations. 10 of them consisted of mixed (20 + 10) passes over normal and inverted data, rest were just single passes over normal data. Tag means that network was fed variations of train set 350 times. With each train set pass taking between as few as 5 seconds (thanks to Theano CUDA/GPU support) up to 60 seconds (when GPU was used also by other students), this was much overall far shorter training time then expected for CIFAR10 solver.\r\n\r\n### Tools\r\nNetwork was written with [Lasagne](http://lasagne.readthedocs.org/en/latest/), a simple Python library for building and training neural networks in Theano. It provides user with ready-to-use variants of most popular layers and training functions. For the purposes of this network, it had everything in place.\r\n\r\n### Hyper parameters\r\nHyper parameters were chosen by trial-and-error, with running multiple versions of network and choosing those that performed best. Initial values were taken from example networks for MNIST from lasagne repository.\r\n\r\n* __Momentum__ equal to `0.9`. This allowed network to initial learn very quickly, but decreased overall accuracy. To counter this, learning rate decay was introduced.\r\n\r\n* __Learning rate__ equal to `0.01*0.1**(iteration_numer/100.0)`. While constant learning rate is often suggested, here it was found that decreasing learning rate with time offered better results.\r\n\r\n* Convolution layers used `32` __learnable convolutional filters__ and `5x5` __filter size__. It was found that increasing filter size actually decreased network accuracy, while increasing filter size made no difference for final accuracy but increased learning time.\r\n\r\n* Max pooling layers used `2x2` __pool size__. With 32x32 images, increasing that parameter was impossible.\r\n\r\n* Dropout layers used __dropout probability__ equal to `0.5` (for deeper layers) and `0.2` (for closer layers).","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}