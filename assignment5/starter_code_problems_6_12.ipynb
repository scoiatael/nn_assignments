{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular network implementation\n",
    "\n",
    "In the following cells, I implement in a modular way a feedforward neural network. Please study the code -- many network implementations follow a similar pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_in, num_out, weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(**kwargs)\n",
    "        if weight_init is None:\n",
    "            #\n",
    "            # TODO propose a default initialization scheme.\n",
    "            # Type a sentence explaining why, and if you use a reference, \n",
    "            # cite it here\n",
    "            # http://stats.stackexchange.com/a/185242\n",
    "            b = numpy.sqrt(6. / (num_in + num_out))\n",
    "            weight_init = Uniform(width=b)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        \n",
    "        self.W = weight_init.generate(self.rng, (num_out, num_in))\n",
    "        self.b = bias_init.generate(self.rng, (num_out, 1))\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        #Save X for later reusal\n",
    "        fprop_context = dict(X=X)\n",
    "        Y = np.dot(self.W, X) +  self.b\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        #\n",
    "        # TODO: fill in gradient computation\n",
    "        #\n",
    "        dYdX = self.W\n",
    "        # print \"Affine: dYdX=\", dYdX.shape, \"dLdY=\", dLdY.shape\n",
    "        return np.dot(dLdY.T, dYdX)\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        dLdW = np.dot(dLdY, X.T)\n",
    "        dLdb = dLdY.sum(1, keepdims=True)\n",
    "        return [dLdW, dLdb]\n",
    "    \n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TanhLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.tanh(X)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        #\n",
    "        # # Fill in proper gradient computation\n",
    "        #\n",
    "        dYdX = (1. - Y**2).T\n",
    "        # print \"Tanh: dYdX=\", dYdX.shape, \"dLdY=\", dLdY.shape\n",
    "        return dYdX.T * dLdY.T\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReLULayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.maximum(X, 0.0)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        return dLdY * (Y>0)\n",
    "\n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def compute_probabilities(self, X):\n",
    "        O = X - X.max(axis=0, keepdims=True)\n",
    "        O = np.exp(O)\n",
    "        O /= O.sum(axis=0, keepdims=True)\n",
    "        return O\n",
    "    \n",
    "    def fprop_cost(self, X, Y):\n",
    "        NS = X.shape[1]\n",
    "        O = self.compute_probabilities(X)\n",
    "        Cost = -1.0/NS * np.log(O[Y.ravel(), range(NS)]).sum()\n",
    "        return Cost, O, dict(O=O, X=X, Y=Y)\n",
    "    \n",
    "    def bprop_cost(self, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        Y = fprop_context['Y']\n",
    "        O = fprop_context['O']\n",
    "        NS = X.shape[1]\n",
    "        dLdX = O.copy()\n",
    "        dLdX[Y, range(NS)] -= 1.0\n",
    "        dLdX /= NS\n",
    "        return dLdX\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "        return self.layers[-1].compute_probabilities(X)\n",
    "    \n",
    "    def get_cost_and_gradient(self, X, Y):\n",
    "        fp_contexts = []\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "            fp_contexts.append(fp_context)\n",
    "        \n",
    "        L, O, fp_context = self.layers[-1].fprop_cost(X, Y)\n",
    "        dLdX = self.layers[-1].bprop_cost(fp_context)\n",
    "        \n",
    "        dLdP = [] #gradient with respect to parameters\n",
    "        for i in xrange(len(self.layers)-1):\n",
    "            layer = self.layers[len(self.layers)-2-i]\n",
    "            fp_context = fp_contexts[len(self.layers)-2-i]\n",
    "            dLdP = layer.get_gradients(dLdX, fp_context) + dLdP\n",
    "            dLdX = layer.bprop(dLdX, fp_context)\n",
    "        return L, O, dLdP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training algorithms. They change the network!\n",
    "def GD(net, X, Y, alpha=1e-4, max_iters=1000000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Simple batch gradient descent\n",
    "    \"\"\"\n",
    "    old_L = np.inf\n",
    "    for i in xrange(max_iters):\n",
    "        L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "        if old_L < L:\n",
    "            print \"Iter: %d, loss increased!!\" % (i,)\n",
    "        # print \"L=\", L, \"diff=\", (old_L - L)\n",
    "        if (old_L - L)<tolerance:\n",
    "            print \"Tolerance level reached exiting\"\n",
    "            break\n",
    "        if i % 1000 == 0:\n",
    "            err_rate = (O.argmax(0) != Y).mean()\n",
    "            print \"At iteration %d, loss %f, loss change %.4g, train error rate %f%%\" % (i, L, old_L - L, err_rate*100)\n",
    "        for P,G in zip(net.parameters, gradients):\n",
    "            P -= alpha * G\n",
    "        old_L = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "IrisX = iris.data.T\n",
    "IrisX = (IrisX - IrisX.mean(axis=1, keepdims=True)) / IrisX.std(axis=1, keepdims=True)\n",
    "IrisY = iris.target.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, loss 1.125784, loss change inf, train error rate 75.333333%\n",
      "At iteration 1000, loss 0.053613, loss change 2.044e-05, train error rate 2.000000%\n",
      "At iteration 2000, loss 0.044182, loss change 4.213e-06, train error rate 1.333333%\n",
      "At iteration 3000, loss 0.041547, loss change 1.638e-06, train error rate 1.333333%\n",
      "At iteration 4000, loss 0.040346, loss change 8.988e-07, train error rate 1.333333%\n",
      "At iteration 5000, loss 0.039597, loss change 6.434e-07, train error rate 1.333333%\n",
      "At iteration 6000, loss 0.039005, loss change 5.551e-07, train error rate 1.333333%\n",
      "At iteration 7000, loss 0.038466, loss change 5.303e-07, train error rate 1.333333%\n",
      "At iteration 8000, loss 0.037936, loss change 5.315e-07, train error rate 1.333333%\n",
      "At iteration 9000, loss 0.037399, loss change 5.451e-07, train error rate 1.333333%\n",
      "At iteration 10000, loss 0.036843, loss change 5.68e-07, train error rate 1.333333%\n",
      "At iteration 11000, loss 0.036258, loss change 6.066e-07, train error rate 1.333333%\n",
      "At iteration 12000, loss 0.035618, loss change 6.812e-07, train error rate 1.333333%\n",
      "At iteration 13000, loss 0.034871, loss change 8.29e-07, train error rate 1.333333%\n",
      "At iteration 14000, loss 0.033917, loss change 1.103e-06, train error rate 1.333333%\n",
      "At iteration 15000, loss 0.032606, loss change 1.547e-06, train error rate 1.333333%\n",
      "At iteration 16000, loss 0.030777, loss change 2.123e-06, train error rate 1.333333%\n",
      "At iteration 17000, loss 0.028372, loss change 2.661e-06, train error rate 1.333333%\n",
      "At iteration 18000, loss 0.025546, loss change 2.934e-06, train error rate 1.333333%\n",
      "At iteration 19000, loss 0.022619, loss change 2.872e-06, train error rate 0.666667%\n",
      "At iteration 20000, loss 0.019875, loss change 2.595e-06, train error rate 0.666667%\n",
      "At iteration 21000, loss 0.017448, loss change 2.257e-06, train error rate 0.666667%\n",
      "At iteration 22000, loss 0.015358, loss change 1.923e-06, train error rate 0.666667%\n",
      "At iteration 23000, loss 0.013600, loss change 1.599e-06, train error rate 0.666667%\n",
      "At iteration 24000, loss 0.012145, loss change 1.323e-06, train error rate 0.000000%\n",
      "At iteration 25000, loss 0.010934, loss change 1.108e-06, train error rate 0.000000%\n",
      "At iteration 26000, loss 0.009914, loss change 9.401e-07, train error rate 0.000000%\n",
      "At iteration 27000, loss 0.009042, loss change 8.069e-07, train error rate 0.000000%\n",
      "At iteration 28000, loss 0.008292, loss change 6.983e-07, train error rate 0.000000%\n",
      "At iteration 29000, loss 0.007639, loss change 6.094e-07, train error rate 0.000000%\n",
      "At iteration 30000, loss 0.007068, loss change 5.355e-07, train error rate 0.000000%\n",
      "At iteration 31000, loss 0.006565, loss change 4.73e-07, train error rate 0.000000%\n",
      "At iteration 32000, loss 0.006119, loss change 4.203e-07, train error rate 0.000000%\n",
      "At iteration 33000, loss 0.005722, loss change 3.753e-07, train error rate 0.000000%\n",
      "At iteration 34000, loss 0.005366, loss change 3.366e-07, train error rate 0.000000%\n",
      "At iteration 35000, loss 0.005047, loss change 3.03e-07, train error rate 0.000000%\n",
      "At iteration 36000, loss 0.004759, loss change 2.741e-07, train error rate 0.000000%\n",
      "At iteration 37000, loss 0.004498, loss change 2.487e-07, train error rate 0.000000%\n",
      "At iteration 38000, loss 0.004260, loss change 2.264e-07, train error rate 0.000000%\n",
      "At iteration 39000, loss 0.004044, loss change 2.065e-07, train error rate 0.000000%\n",
      "At iteration 40000, loss 0.003846, loss change 1.894e-07, train error rate 0.000000%\n",
      "At iteration 41000, loss 0.003665, loss change 1.737e-07, train error rate 0.000000%\n",
      "At iteration 42000, loss 0.003498, loss change 1.6e-07, train error rate 0.000000%\n",
      "At iteration 43000, loss 0.003345, loss change 1.476e-07, train error rate 0.000000%\n",
      "At iteration 44000, loss 0.003203, loss change 1.367e-07, train error rate 0.000000%\n",
      "At iteration 45000, loss 0.003071, loss change 1.265e-07, train error rate 0.000000%\n",
      "At iteration 46000, loss 0.002949, loss change 1.178e-07, train error rate 0.000000%\n",
      "At iteration 47000, loss 0.002836, loss change 1.097e-07, train error rate 0.000000%\n",
      "At iteration 48000, loss 0.002730, loss change 1.023e-07, train error rate 0.000000%\n",
      "Tolerance level reached exiting\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we verify that the network can be trained on Irises.\n",
    "# Most runs should result in 100% accurracy\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(4,10),\n",
    "        TanhLayer(),\n",
    "        AffineLayer(10,3),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "GD(net, IrisX,IrisY, 1e-1, tolerance=1e-7, max_iters=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD\n",
    "\n",
    "Your job is to implement SGD training on MNIST with the following elements:\n",
    "1. SGD + momentum\n",
    "2. weight decay\n",
    "3. early stopping\n",
    "\n",
    "In overall, you should get below 2% trainig errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from Fuel\n",
    "\n",
    "The following cell prepares the data pipeline in fuel. please see SGD template for usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "MNIST.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}), \n",
    "    (Flatten, [], {'which_sources': 'features'}),\n",
    "    (Mapping, [lambda batch: (b.T for b in batch)], {}) )\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 100))\n",
    "                                               \n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 250))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (784L, 100L) containing float32\n",
      " - an array of size (1L, 100L) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (784L, 250L) containing float32\n",
      " - an array of size (1L, 250L) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (mnist_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(mnist_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(mnist_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Please note, the code blow is able to train a SoftMax regression model on mnist to poor results (ca 8%test error), \n",
    "# you must improve it\n",
    "#\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def compute_error_rate(net, stream):\n",
    "    num_errs = 0.0\n",
    "    num_examples = 0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        O = net.fprop(X)\n",
    "        num_errs += (O.argmax(0) != Y).sum()\n",
    "        num_examples += X.shape[1]\n",
    "    return num_errs/num_examples\n",
    "\n",
    "def SGD(net, train_stream, validation_stream, test_stream):\n",
    "    i=0\n",
    "    e=0\n",
    "    \n",
    "    #initialize momentum variables\n",
    "    #\n",
    "    # TODO\n",
    "    #\n",
    "    # Hint: you need one valocity matrix for each parameter\n",
    "    velocities = [None for P in net.parameters]\n",
    "    \n",
    "    best_valid_error_rate = np.inf\n",
    "    best_params = deepcopy(net.parameters)\n",
    "    best_params_epoch = 0\n",
    "    \n",
    "    train_erros = []\n",
    "    train_loss = []\n",
    "    validation_errors = []\n",
    "    \n",
    "    number_of_epochs = 3\n",
    "    patience_expansion = 1.5\n",
    "    \n",
    "    try:\n",
    "        while e<number_of_epochs: #This loop goes over epochs\n",
    "            e += 1\n",
    "            #First train on all data from this batch\n",
    "            for X,Y in train_stream.get_epoch_iterator(): \n",
    "                i += 1\n",
    "                L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "                err_rate = (O.argmax(0) != Y).mean()\n",
    "                train_loss.append((i,L))\n",
    "                train_erros.append((i,err_rate))\n",
    "                if i % 100 == 0:\n",
    "                    print \"At minibatch %d, batch loss %f, batch error rate %f%%\" % (i, L, err_rate*100)\n",
    "                for P, V, G, N in zip(net.parameters, velocities, gradients, net.parameter_names):\n",
    "                    if N=='W':\n",
    "                        pass\n",
    "                        #\n",
    "                        # TODO: implement the weight decay addition to gradient\n",
    "                        #\n",
    "                        #G += TODO\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: set a learning rate\n",
    "                    #\n",
    "                    # Hint, use the iteration counter i\n",
    "                    # alpha = TODO\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: set the momentum constant \n",
    "                    # \n",
    "                    \n",
    "                    # epsilon = TODO\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: implement velocity update in momentum\n",
    "                    #\n",
    "                    \n",
    "                    # V[...] = TODO\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: set a more sensible learning rule here,\n",
    "                    # using your learning rate schedule and momentum\n",
    "                    #\n",
    "                    #!!!!! Need to modify the actual parameter here! \n",
    "                    P += -5e-2 * G\n",
    "            # After an epoch compute validation error\n",
    "            val_error_rate = compute_error_rate(net, validation_stream)\n",
    "            if val_error_rate < best_valid_error_rate:\n",
    "                number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "                best_valid_error_rate = val_error_rate\n",
    "                best_params = deepcopy(net.parameters)\n",
    "                best_params_epoch = e\n",
    "                validation_errors.append((i,val_error_rate))\n",
    "            print \"After epoch %d: valid_err_rate: %f%% currently going ot do %d epochs\" %(\n",
    "                e, val_error_rate, number_of_epochs)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print \"Setting network parameters from after epoch %d\" %(best_params_epoch)\n",
    "        net.parameters = best_params\n",
    "        \n",
    "        subplot(2,1,1)\n",
    "        train_loss = np.array(train_loss)\n",
    "        semilogy(train_loss[:,0], train_loss[:,1], label='batch train loss')\n",
    "        legend()\n",
    "        \n",
    "        subplot(2,1,2)\n",
    "        train_erros = np.array(train_erros)\n",
    "        plot(train_erros[:,0], train_erros[:,1], label='batch train error rate')\n",
    "        validation_errors = np.array(validation_errors)\n",
    "        plot(validation_errors[:,0], validation_errors[:,1], label='validation error rate', color='r')\n",
    "        ylim(0,0.2)\n",
    "        legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 0.605426, batch error rate 19.000000%\n",
      "At minibatch 200, batch loss 0.401861, batch error rate 10.000000%\n",
      "At minibatch 300, batch loss 0.458826, batch error rate 17.000000%\n",
      "At minibatch 400, batch loss 0.297689, batch error rate 10.000000%\n",
      "At minibatch 500, batch loss 0.307453, batch error rate 9.000000%\n",
      "After epoch 1: valid_err_rate: 0.095200% currently going ot do 3 epochs\n",
      "At minibatch 600, batch loss 0.272669, batch error rate 10.000000%\n",
      "At minibatch 700, batch loss 0.228750, batch error rate 7.000000%\n",
      "At minibatch 800, batch loss 0.334208, batch error rate 9.000000%\n",
      "At minibatch 900, batch loss 0.393104, batch error rate 11.000000%\n",
      "At minibatch 1000, batch loss 0.386667, batch error rate 11.000000%\n",
      "After epoch 2: valid_err_rate: 0.092800% currently going ot do 4 epochs\n",
      "At minibatch 1100, batch loss 0.289695, batch error rate 4.000000%\n",
      "At minibatch 1200, batch loss 0.329239, batch error rate 8.000000%\n",
      "At minibatch 1300, batch loss 0.357242, batch error rate 9.000000%\n",
      "At minibatch 1400, batch loss 0.276767, batch error rate 6.000000%\n",
      "At minibatch 1500, batch loss 0.298512, batch error rate 10.000000%\n",
      "After epoch 3: valid_err_rate: 0.083100% currently going ot do 5 epochs\n",
      "At minibatch 1600, batch loss 0.463534, batch error rate 14.000000%\n",
      "At minibatch 1700, batch loss 0.271772, batch error rate 10.000000%\n",
      "At minibatch 1800, batch loss 0.320582, batch error rate 13.000000%\n",
      "At minibatch 1900, batch loss 0.300154, batch error rate 9.000000%\n",
      "At minibatch 2000, batch loss 0.542977, batch error rate 11.000000%\n",
      "After epoch 4: valid_err_rate: 0.086500% currently going ot do 5 epochs\n",
      "At minibatch 2100, batch loss 0.536634, batch error rate 10.000000%\n",
      "At minibatch 2200, batch loss 0.313223, batch error rate 6.000000%\n",
      "At minibatch 2300, batch loss 0.388835, batch error rate 13.000000%\n",
      "At minibatch 2400, batch loss 0.433575, batch error rate 10.000000%\n",
      "At minibatch 2500, batch loss 0.358583, batch error rate 9.000000%\n",
      "After epoch 5: valid_err_rate: 0.080800% currently going ot do 8 epochs\n",
      "At minibatch 2600, batch loss 0.194612, batch error rate 8.000000%\n",
      "At minibatch 2700, batch loss 0.349266, batch error rate 9.000000%\n",
      "At minibatch 2800, batch loss 0.413949, batch error rate 16.000000%\n",
      "At minibatch 2900, batch loss 0.378393, batch error rate 8.000000%\n",
      "At minibatch 3000, batch loss 0.254977, batch error rate 6.000000%\n",
      "After epoch 6: valid_err_rate: 0.085100% currently going ot do 8 epochs\n",
      "At minibatch 3100, batch loss 0.441699, batch error rate 14.000000%\n",
      "At minibatch 3200, batch loss 0.384577, batch error rate 10.000000%\n",
      "At minibatch 3300, batch loss 0.335284, batch error rate 13.000000%\n",
      "At minibatch 3400, batch loss 0.349344, batch error rate 10.000000%\n",
      "At minibatch 3500, batch loss 0.186212, batch error rate 6.000000%\n",
      "After epoch 7: valid_err_rate: 0.078800% currently going ot do 11 epochs\n",
      "At minibatch 3600, batch loss 0.305836, batch error rate 10.000000%\n",
      "At minibatch 3700, batch loss 0.319814, batch error rate 11.000000%\n",
      "At minibatch 3800, batch loss 0.345522, batch error rate 9.000000%\n",
      "At minibatch 3900, batch loss 0.236212, batch error rate 6.000000%\n",
      "At minibatch 4000, batch loss 0.409615, batch error rate 15.000000%\n",
      "After epoch 8: valid_err_rate: 0.077500% currently going ot do 13 epochs\n",
      "At minibatch 4100, batch loss 0.334983, batch error rate 11.000000%\n",
      "At minibatch 4200, batch loss 0.404449, batch error rate 12.000000%\n",
      "At minibatch 4300, batch loss 0.225488, batch error rate 8.000000%\n",
      "At minibatch 4400, batch loss 0.135158, batch error rate 3.000000%\n",
      "At minibatch 4500, batch loss 0.277079, batch error rate 10.000000%\n",
      "After epoch 9: valid_err_rate: 0.076900% currently going ot do 14 epochs\n",
      "At minibatch 4600, batch loss 0.592702, batch error rate 12.000000%\n",
      "At minibatch 4700, batch loss 0.171974, batch error rate 8.000000%\n",
      "At minibatch 4800, batch loss 0.203155, batch error rate 4.000000%\n",
      "At minibatch 4900, batch loss 0.296432, batch error rate 6.000000%\n",
      "At minibatch 5000, batch loss 0.134087, batch error rate 3.000000%\n",
      "After epoch 10: valid_err_rate: 0.075300% currently going ot do 16 epochs\n",
      "At minibatch 5100, batch loss 0.199339, batch error rate 9.000000%\n",
      "At minibatch 5200, batch loss 0.261371, batch error rate 5.000000%\n",
      "At minibatch 5300, batch loss 0.451680, batch error rate 12.000000%\n",
      "At minibatch 5400, batch loss 0.334797, batch error rate 8.000000%\n",
      "At minibatch 5500, batch loss 0.365219, batch error rate 9.000000%\n",
      "After epoch 11: valid_err_rate: 0.077400% currently going ot do 16 epochs\n",
      "At minibatch 5600, batch loss 0.263835, batch error rate 9.000000%\n",
      "At minibatch 5700, batch loss 0.280778, batch error rate 10.000000%\n",
      "At minibatch 5800, batch loss 0.241067, batch error rate 7.000000%\n",
      "At minibatch 5900, batch loss 0.334669, batch error rate 8.000000%\n",
      "At minibatch 6000, batch loss 0.356102, batch error rate 7.000000%\n",
      "After epoch 12: valid_err_rate: 0.079400% currently going ot do 16 epochs\n",
      "At minibatch 6100, batch loss 0.206948, batch error rate 4.000000%\n",
      "At minibatch 6200, batch loss 0.383033, batch error rate 13.000000%\n",
      "At minibatch 6300, batch loss 0.322988, batch error rate 7.000000%\n",
      "At minibatch 6400, batch loss 0.149795, batch error rate 3.000000%\n",
      "At minibatch 6500, batch loss 0.309678, batch error rate 8.000000%\n",
      "After epoch 13: valid_err_rate: 0.079900% currently going ot do 16 epochs\n",
      "At minibatch 6600, batch loss 0.235722, batch error rate 6.000000%\n",
      "At minibatch 6700, batch loss 0.356123, batch error rate 10.000000%\n",
      "At minibatch 6800, batch loss 0.143357, batch error rate 2.000000%\n",
      "At minibatch 6900, batch loss 0.329140, batch error rate 10.000000%\n",
      "At minibatch 7000, batch loss 0.164363, batch error rate 3.000000%\n",
      "After epoch 14: valid_err_rate: 0.078600% currently going ot do 16 epochs\n",
      "At minibatch 7100, batch loss 0.221073, batch error rate 6.000000%\n",
      "At minibatch 7200, batch loss 0.410840, batch error rate 10.000000%\n",
      "At minibatch 7300, batch loss 0.301068, batch error rate 11.000000%\n",
      "At minibatch 7400, batch loss 0.408358, batch error rate 10.000000%\n",
      "At minibatch 7500, batch loss 0.326963, batch error rate 6.000000%\n",
      "After epoch 15: valid_err_rate: 0.072700% currently going ot do 23 epochs\n",
      "At minibatch 7600, batch loss 0.427065, batch error rate 11.000000%\n",
      "At minibatch 7700, batch loss 0.145518, batch error rate 7.000000%\n",
      "At minibatch 7800, batch loss 0.284450, batch error rate 9.000000%\n",
      "At minibatch 7900, batch loss 0.492255, batch error rate 15.000000%\n",
      "At minibatch 8000, batch loss 0.301231, batch error rate 6.000000%\n",
      "After epoch 16: valid_err_rate: 0.073400% currently going ot do 23 epochs\n",
      "At minibatch 8100, batch loss 0.318193, batch error rate 8.000000%\n",
      "At minibatch 8200, batch loss 0.220939, batch error rate 9.000000%\n",
      "At minibatch 8300, batch loss 0.243513, batch error rate 6.000000%\n",
      "At minibatch 8400, batch loss 0.220577, batch error rate 5.000000%\n",
      "At minibatch 8500, batch loss 0.263398, batch error rate 9.000000%\n",
      "After epoch 17: valid_err_rate: 0.077900% currently going ot do 23 epochs\n",
      "At minibatch 8600, batch loss 0.294638, batch error rate 8.000000%\n",
      "At minibatch 8700, batch loss 0.360745, batch error rate 10.000000%\n",
      "At minibatch 8800, batch loss 0.201914, batch error rate 7.000000%\n",
      "At minibatch 8900, batch loss 0.325593, batch error rate 9.000000%\n",
      "At minibatch 9000, batch loss 0.369629, batch error rate 9.000000%\n",
      "After epoch 18: valid_err_rate: 0.076400% currently going ot do 23 epochs\n",
      "At minibatch 9100, batch loss 0.125674, batch error rate 3.000000%\n",
      "At minibatch 9200, batch loss 0.191639, batch error rate 4.000000%\n",
      "At minibatch 9300, batch loss 0.274363, batch error rate 7.000000%\n",
      "At minibatch 9400, batch loss 0.244925, batch error rate 7.000000%\n",
      "At minibatch 9500, batch loss 0.224735, batch error rate 8.000000%\n",
      "After epoch 19: valid_err_rate: 0.080200% currently going ot do 23 epochs\n",
      "At minibatch 9600, batch loss 0.357606, batch error rate 9.000000%\n",
      "At minibatch 9700, batch loss 0.481631, batch error rate 12.000000%\n",
      "At minibatch 9800, batch loss 0.505595, batch error rate 11.000000%\n",
      "At minibatch 9900, batch loss 0.266829, batch error rate 8.000000%\n",
      "At minibatch 10000, batch loss 0.265320, batch error rate 9.000000%\n",
      "After epoch 20: valid_err_rate: 0.075900% currently going ot do 23 epochs\n",
      "At minibatch 10100, batch loss 0.359045, batch error rate 7.000000%\n",
      "At minibatch 10200, batch loss 0.340120, batch error rate 9.000000%\n",
      "At minibatch 10300, batch loss 0.259127, batch error rate 8.000000%\n",
      "At minibatch 10400, batch loss 0.275457, batch error rate 4.000000%\n",
      "At minibatch 10500, batch loss 0.296282, batch error rate 7.000000%\n",
      "After epoch 21: valid_err_rate: 0.074800% currently going ot do 23 epochs\n",
      "At minibatch 10600, batch loss 0.212020, batch error rate 6.000000%\n",
      "At minibatch 10700, batch loss 0.105605, batch error rate 5.000000%\n",
      "At minibatch 10800, batch loss 0.466182, batch error rate 10.000000%\n",
      "At minibatch 10900, batch loss 0.413703, batch error rate 12.000000%\n",
      "At minibatch 11000, batch loss 0.232462, batch error rate 5.000000%\n",
      "After epoch 22: valid_err_rate: 0.076600% currently going ot do 23 epochs\n",
      "At minibatch 11100, batch loss 0.196455, batch error rate 6.000000%\n",
      "At minibatch 11200, batch loss 0.338046, batch error rate 8.000000%\n",
      "At minibatch 11300, batch loss 0.186840, batch error rate 6.000000%\n",
      "At minibatch 11400, batch loss 0.267710, batch error rate 7.000000%\n",
      "At minibatch 11500, batch loss 0.153099, batch error rate 5.000000%\n",
      "After epoch 23: valid_err_rate: 0.071600% currently going ot do 35 epochs\n",
      "At minibatch 11600, batch loss 0.171866, batch error rate 5.000000%\n",
      "At minibatch 11700, batch loss 0.350653, batch error rate 11.000000%\n",
      "At minibatch 11800, batch loss 0.429318, batch error rate 11.000000%\n",
      "At minibatch 11900, batch loss 0.315289, batch error rate 8.000000%\n",
      "At minibatch 12000, batch loss 0.250522, batch error rate 9.000000%\n",
      "After epoch 24: valid_err_rate: 0.078100% currently going ot do 35 epochs\n",
      "At minibatch 12100, batch loss 0.092787, batch error rate 1.000000%\n",
      "At minibatch 12200, batch loss 0.348897, batch error rate 10.000000%\n",
      "At minibatch 12300, batch loss 0.267251, batch error rate 6.000000%\n",
      "At minibatch 12400, batch loss 0.246117, batch error rate 8.000000%\n",
      "At minibatch 12500, batch loss 0.262392, batch error rate 8.000000%\n",
      "After epoch 25: valid_err_rate: 0.078000% currently going ot do 35 epochs\n",
      "At minibatch 12600, batch loss 0.233412, batch error rate 4.000000%\n",
      "At minibatch 12700, batch loss 0.109909, batch error rate 3.000000%\n",
      "At minibatch 12800, batch loss 0.241778, batch error rate 10.000000%\n",
      "At minibatch 12900, batch loss 0.210184, batch error rate 7.000000%\n",
      "At minibatch 13000, batch loss 0.206299, batch error rate 5.000000%\n",
      "After epoch 26: valid_err_rate: 0.073800% currently going ot do 35 epochs\n",
      "At minibatch 13100, batch loss 0.139435, batch error rate 3.000000%\n",
      "At minibatch 13200, batch loss 0.340849, batch error rate 8.000000%\n",
      "At minibatch 13300, batch loss 0.152212, batch error rate 7.000000%\n",
      "At minibatch 13400, batch loss 0.315396, batch error rate 11.000000%\n",
      "At minibatch 13500, batch loss 0.371889, batch error rate 12.000000%\n",
      "After epoch 27: valid_err_rate: 0.076600% currently going ot do 35 epochs\n",
      "At minibatch 13600, batch loss 0.372271, batch error rate 9.000000%\n",
      "At minibatch 13700, batch loss 0.496586, batch error rate 9.000000%\n",
      "At minibatch 13800, batch loss 0.309182, batch error rate 10.000000%\n",
      "At minibatch 13900, batch loss 0.245134, batch error rate 7.000000%\n",
      "At minibatch 14000, batch loss 0.290933, batch error rate 9.000000%\n",
      "After epoch 28: valid_err_rate: 0.076100% currently going ot do 35 epochs\n",
      "At minibatch 14100, batch loss 0.212285, batch error rate 9.000000%\n",
      "At minibatch 14200, batch loss 0.557672, batch error rate 15.000000%\n",
      "At minibatch 14300, batch loss 0.350569, batch error rate 6.000000%\n",
      "At minibatch 14400, batch loss 0.262446, batch error rate 9.000000%\n",
      "At minibatch 14500, batch loss 0.221544, batch error rate 7.000000%\n",
      "After epoch 29: valid_err_rate: 0.074500% currently going ot do 35 epochs\n",
      "At minibatch 14600, batch loss 0.300918, batch error rate 11.000000%\n",
      "At minibatch 14700, batch loss 0.293884, batch error rate 11.000000%\n",
      "At minibatch 14800, batch loss 0.311457, batch error rate 7.000000%\n",
      "At minibatch 14900, batch loss 0.214935, batch error rate 5.000000%\n",
      "At minibatch 15000, batch loss 0.365468, batch error rate 6.000000%\n",
      "After epoch 30: valid_err_rate: 0.075500% currently going ot do 35 epochs\n",
      "At minibatch 15100, batch loss 0.189664, batch error rate 6.000000%\n",
      "At minibatch 15200, batch loss 0.274012, batch error rate 8.000000%\n",
      "At minibatch 15300, batch loss 0.310068, batch error rate 9.000000%\n",
      "At minibatch 15400, batch loss 0.327802, batch error rate 9.000000%\n",
      "At minibatch 15500, batch loss 0.359011, batch error rate 10.000000%\n",
      "After epoch 31: valid_err_rate: 0.073400% currently going ot do 35 epochs\n",
      "At minibatch 15600, batch loss 0.233860, batch error rate 7.000000%\n",
      "At minibatch 15700, batch loss 0.284283, batch error rate 6.000000%\n",
      "At minibatch 15800, batch loss 0.245900, batch error rate 8.000000%\n",
      "At minibatch 15900, batch loss 0.321281, batch error rate 9.000000%\n",
      "At minibatch 16000, batch loss 0.356172, batch error rate 9.000000%\n",
      "After epoch 32: valid_err_rate: 0.075600% currently going ot do 35 epochs\n",
      "At minibatch 16100, batch loss 0.414589, batch error rate 14.000000%\n",
      "At minibatch 16200, batch loss 0.329618, batch error rate 9.000000%\n",
      "At minibatch 16300, batch loss 0.144683, batch error rate 3.000000%\n",
      "At minibatch 16400, batch loss 0.137833, batch error rate 2.000000%\n",
      "At minibatch 16500, batch loss 0.241034, batch error rate 8.000000%\n",
      "After epoch 33: valid_err_rate: 0.076200% currently going ot do 35 epochs\n",
      "At minibatch 16600, batch loss 0.219007, batch error rate 5.000000%\n",
      "At minibatch 16700, batch loss 0.174595, batch error rate 7.000000%\n",
      "At minibatch 16800, batch loss 0.115410, batch error rate 2.000000%\n",
      "At minibatch 16900, batch loss 0.347303, batch error rate 8.000000%\n",
      "At minibatch 17000, batch loss 0.164725, batch error rate 4.000000%\n",
      "After epoch 34: valid_err_rate: 0.075000% currently going ot do 35 epochs\n",
      "At minibatch 17100, batch loss 0.529644, batch error rate 13.000000%\n",
      "At minibatch 17200, batch loss 0.269042, batch error rate 9.000000%\n",
      "At minibatch 17300, batch loss 0.237759, batch error rate 6.000000%\n",
      "At minibatch 17400, batch loss 0.165119, batch error rate 6.000000%\n",
      "At minibatch 17500, batch loss 0.187916, batch error rate 8.000000%\n",
      "After epoch 35: valid_err_rate: 0.072300% currently going ot do 35 epochs\n",
      "At minibatch 17600, batch loss 0.182968, batch error rate 6.000000%\n",
      "At minibatch 17700, batch loss 0.234444, batch error rate 8.000000%\n",
      "At minibatch 17800, batch loss 0.315086, batch error rate 11.000000%\n",
      "At minibatch 17900, batch loss 0.233076, batch error rate 6.000000%\n",
      "At minibatch 18000, batch loss 0.309586, batch error rate 10.000000%\n",
      "After epoch 36: valid_err_rate: 0.074000% currently going ot do 35 epochs\n",
      "Test error rate: 0.076400\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# TODO: pick a network architecture here. The one below is just \n",
    "# softmax regression\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(784,10),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "SGD(net, mnist_train_stream, mnist_validation_stream, mnist_test_stream)\n",
    "\n",
    "print \"Test error rate: %f\" % (compute_error_rate(net, mnist_test_stream), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_error_rate(net, mnist_test_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
