{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular network implementation\n",
    "\n",
    "In the following cells, I implement in a modular way a feedforward neural network. Please study the code -- many network implementations follow a similar pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_in, num_out, weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(**kwargs)\n",
    "        if weight_init is None:\n",
    "            #\n",
    "            # TODO propose a default initialization scheme.\n",
    "            # Type a sentence explaining why, and if you use a reference, \n",
    "            # cite it here\n",
    "            # http://stats.stackexchange.com/a/185242\n",
    "            b = numpy.sqrt(6. / (num_in + num_out))\n",
    "            weight_init = Uniform(width=b)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        \n",
    "        self.W = weight_init.generate(self.rng, (num_out, num_in))\n",
    "        self.b = bias_init.generate(self.rng, (num_out, 1))\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        #Save X for later reusal\n",
    "        fprop_context = dict(X=X)\n",
    "        Y = np.dot(self.W, X) +  self.b\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        #\n",
    "        # TODO: fill in gradient computation\n",
    "        #\n",
    "        dYdX = self.W\n",
    "        # print \"Affine: dYdX=\", dYdX.shape, \"dLdY=\", dLdY.shape\n",
    "        return np.dot(dLdY.T, dYdX)\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        dLdW = np.dot(dLdY, X.T)\n",
    "        dLdb = dLdY.sum(1, keepdims=True)\n",
    "        return [dLdW, dLdb]\n",
    "    \n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TanhLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.tanh(X)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        #\n",
    "        # # Fill in proper gradient computation\n",
    "        #\n",
    "        dYdX = (1. - Y**2).T\n",
    "        # print \"Tanh: dYdX=\", dYdX.shape, \"dLdY=\", dLdY.shape\n",
    "        return dYdX.T * dLdY.T\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReLULayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.maximum(X, 0.0)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        return dLdY.T * (Y>0)\n",
    "\n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def compute_probabilities(self, X):\n",
    "        O = X - X.max(axis=0, keepdims=True)\n",
    "        O = np.exp(O)\n",
    "        O /= O.sum(axis=0, keepdims=True)\n",
    "        return O\n",
    "    \n",
    "    def fprop_cost(self, X, Y):\n",
    "        NS = X.shape[1]\n",
    "        O = self.compute_probabilities(X)\n",
    "        Cost = -1.0/NS * np.log(O[Y.ravel(), range(NS)]).sum()\n",
    "        return Cost, O, dict(O=O, X=X, Y=Y)\n",
    "    \n",
    "    def bprop_cost(self, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        Y = fprop_context['Y']\n",
    "        O = fprop_context['O']\n",
    "        NS = X.shape[1]\n",
    "        dLdX = O.copy()\n",
    "        dLdX[Y, range(NS)] -= 1.0\n",
    "        dLdX /= NS\n",
    "        return dLdX\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "        return self.layers[-1].compute_probabilities(X)\n",
    "    \n",
    "    def get_cost_and_gradient(self, X, Y):\n",
    "        fp_contexts = []\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "            fp_contexts.append(fp_context)\n",
    "        \n",
    "        L, O, fp_context = self.layers[-1].fprop_cost(X, Y)\n",
    "        dLdX = self.layers[-1].bprop_cost(fp_context)\n",
    "        \n",
    "        dLdP = [] #gradient with respect to parameters\n",
    "        for i in xrange(len(self.layers)-1):\n",
    "            layer = self.layers[len(self.layers)-2-i]\n",
    "            fp_context = fp_contexts[len(self.layers)-2-i]\n",
    "            dLdP = layer.get_gradients(dLdX, fp_context) + dLdP\n",
    "            dLdX = layer.bprop(dLdX, fp_context)\n",
    "        return L, O, dLdP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training algorithms. They change the network!\n",
    "def GD(net, X, Y, alpha=1e-4, max_iters=1000000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Simple batch gradient descent\n",
    "    \"\"\"\n",
    "    old_L = np.inf\n",
    "    for i in xrange(max_iters):\n",
    "        L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "        if old_L < L:\n",
    "            print \"Iter: %d, loss increased!!\" % (i,)\n",
    "        # print \"L=\", L, \"diff=\", (old_L - L)\n",
    "        if (old_L - L)<tolerance:\n",
    "            print \"Tolerance level reached exiting\"\n",
    "            break\n",
    "        if i % 1000 == 0:\n",
    "            err_rate = (O.argmax(0) != Y).mean()\n",
    "            print \"At iteration %d, loss %f, loss change %.4g, train error rate %f%%\" % (i, L, old_L - L, err_rate*100)\n",
    "        for P,G in zip(net.parameters, gradients):\n",
    "            P -= alpha * G\n",
    "        old_L = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "IrisX = iris.data.T\n",
    "IrisX = (IrisX - IrisX.mean(axis=1, keepdims=True)) / IrisX.std(axis=1, keepdims=True)\n",
    "IrisY = iris.target.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, loss 1.125784, loss change inf, train error rate 75.333333%\n",
      "At iteration 1000, loss 0.053613, loss change 2.044e-05, train error rate 2.000000%\n",
      "At iteration 2000, loss 0.044182, loss change 4.213e-06, train error rate 1.333333%\n",
      "At iteration 3000, loss 0.041547, loss change 1.638e-06, train error rate 1.333333%\n",
      "At iteration 4000, loss 0.040346, loss change 8.988e-07, train error rate 1.333333%\n",
      "At iteration 5000, loss 0.039597, loss change 6.434e-07, train error rate 1.333333%\n",
      "At iteration 6000, loss 0.039005, loss change 5.551e-07, train error rate 1.333333%\n",
      "At iteration 7000, loss 0.038466, loss change 5.303e-07, train error rate 1.333333%\n",
      "At iteration 8000, loss 0.037936, loss change 5.315e-07, train error rate 1.333333%\n",
      "At iteration 9000, loss 0.037399, loss change 5.451e-07, train error rate 1.333333%\n",
      "At iteration 10000, loss 0.036843, loss change 5.68e-07, train error rate 1.333333%\n",
      "At iteration 11000, loss 0.036258, loss change 6.066e-07, train error rate 1.333333%\n",
      "At iteration 12000, loss 0.035618, loss change 6.812e-07, train error rate 1.333333%\n",
      "At iteration 13000, loss 0.034871, loss change 8.29e-07, train error rate 1.333333%\n",
      "At iteration 14000, loss 0.033917, loss change 1.103e-06, train error rate 1.333333%\n",
      "At iteration 15000, loss 0.032606, loss change 1.547e-06, train error rate 1.333333%\n",
      "At iteration 16000, loss 0.030777, loss change 2.123e-06, train error rate 1.333333%\n",
      "At iteration 17000, loss 0.028372, loss change 2.661e-06, train error rate 1.333333%\n",
      "At iteration 18000, loss 0.025546, loss change 2.934e-06, train error rate 1.333333%\n",
      "At iteration 19000, loss 0.022619, loss change 2.872e-06, train error rate 0.666667%\n",
      "At iteration 20000, loss 0.019875, loss change 2.595e-06, train error rate 0.666667%\n",
      "At iteration 21000, loss 0.017448, loss change 2.257e-06, train error rate 0.666667%\n",
      "At iteration 22000, loss 0.015358, loss change 1.923e-06, train error rate 0.666667%\n",
      "At iteration 23000, loss 0.013600, loss change 1.599e-06, train error rate 0.666667%\n",
      "At iteration 24000, loss 0.012145, loss change 1.323e-06, train error rate 0.000000%\n",
      "At iteration 25000, loss 0.010934, loss change 1.108e-06, train error rate 0.000000%\n",
      "At iteration 26000, loss 0.009914, loss change 9.401e-07, train error rate 0.000000%\n",
      "At iteration 27000, loss 0.009042, loss change 8.069e-07, train error rate 0.000000%\n",
      "At iteration 28000, loss 0.008292, loss change 6.983e-07, train error rate 0.000000%\n",
      "At iteration 29000, loss 0.007639, loss change 6.094e-07, train error rate 0.000000%\n",
      "At iteration 30000, loss 0.007068, loss change 5.355e-07, train error rate 0.000000%\n",
      "At iteration 31000, loss 0.006565, loss change 4.73e-07, train error rate 0.000000%\n",
      "At iteration 32000, loss 0.006119, loss change 4.203e-07, train error rate 0.000000%\n",
      "At iteration 33000, loss 0.005722, loss change 3.753e-07, train error rate 0.000000%\n",
      "At iteration 34000, loss 0.005366, loss change 3.366e-07, train error rate 0.000000%\n",
      "At iteration 35000, loss 0.005047, loss change 3.03e-07, train error rate 0.000000%\n",
      "At iteration 36000, loss 0.004759, loss change 2.741e-07, train error rate 0.000000%\n",
      "At iteration 37000, loss 0.004498, loss change 2.487e-07, train error rate 0.000000%\n",
      "At iteration 38000, loss 0.004260, loss change 2.264e-07, train error rate 0.000000%\n",
      "At iteration 39000, loss 0.004044, loss change 2.065e-07, train error rate 0.000000%\n",
      "At iteration 40000, loss 0.003846, loss change 1.894e-07, train error rate 0.000000%\n",
      "At iteration 41000, loss 0.003665, loss change 1.737e-07, train error rate 0.000000%\n",
      "At iteration 42000, loss 0.003498, loss change 1.6e-07, train error rate 0.000000%\n",
      "At iteration 43000, loss 0.003345, loss change 1.476e-07, train error rate 0.000000%\n",
      "At iteration 44000, loss 0.003203, loss change 1.367e-07, train error rate 0.000000%\n",
      "At iteration 45000, loss 0.003071, loss change 1.265e-07, train error rate 0.000000%\n",
      "At iteration 46000, loss 0.002949, loss change 1.178e-07, train error rate 0.000000%\n",
      "At iteration 47000, loss 0.002836, loss change 1.097e-07, train error rate 0.000000%\n",
      "At iteration 48000, loss 0.002730, loss change 1.023e-07, train error rate 0.000000%\n",
      "Tolerance level reached exiting\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we verify that the network can be trained on Irises.\n",
    "# Most runs should result in 100% accurracy\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(4,10),\n",
    "        TanhLayer(),\n",
    "        AffineLayer(10,3),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "GD(net, IrisX,IrisY, 1e-1, tolerance=1e-7, max_iters=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD\n",
    "\n",
    "Your job is to implement SGD training on MNIST with the following elements:\n",
    "1. SGD + momentum\n",
    "2. weight decay\n",
    "3. early stopping\n",
    "\n",
    "In overall, you should get below 2% trainig errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from Fuel\n",
    "\n",
    "The following cell prepares the data pipeline in fuel. please see SGD template for usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "MNIST.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}), \n",
    "    (Flatten, [], {'which_sources': 'features'}),\n",
    "    (Mapping, [lambda batch: (b.T for b in batch)], {}) )\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 100))\n",
    "                                               \n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 250))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (784, 100) containing float32\n",
      " - an array of size (1, 100) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (784, 250) containing float32\n",
      " - an array of size (1, 250) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (mnist_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(mnist_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(mnist_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Please note, the code blow is able to train a SoftMax regression model on mnist to poor results (ca 8%test error), \n",
    "# you must improve it\n",
    "#\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def compute_error_rate(net, stream):\n",
    "    num_errs = 0.0\n",
    "    num_examples = 0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        O = net.fprop(X)\n",
    "        num_errs += (O.argmax(0) != Y).sum()\n",
    "        num_examples += X.shape[1]\n",
    "    return num_errs/num_examples\n",
    "\n",
    "def SGD(net, train_stream, validation_stream, test_stream, epsilon=5e-2, alp=(5e-1, 5e-3), lam=1e-4, pat=1.4, alp_dropout=20000):\n",
    "    i=0\n",
    "    e=0\n",
    "    \n",
    "    #initialize momentum variables\n",
    "    #\n",
    "    # TODO\n",
    "    #\n",
    "    # Hint: you need one valocity matrix for each parameter\n",
    "    velocities = [None for P in net.parameters]\n",
    "    \n",
    "    best_valid_error_rate = np.inf\n",
    "    best_params = deepcopy(net.parameters)\n",
    "    best_params_epoch = 0\n",
    "    \n",
    "    train_erros = []\n",
    "    train_loss = []\n",
    "    validation_errors = []\n",
    "    \n",
    "    number_of_epochs = 10\n",
    "    patience_expansion = pat\n",
    "    \n",
    "    try:\n",
    "        while e<number_of_epochs: #This loop goes over epochs\n",
    "            e += 1\n",
    "            #First train on all data from this batch\n",
    "            for X,Y in train_stream.get_epoch_iterator(): \n",
    "                i += 1\n",
    "                L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "                err_rate = (O.argmax(0) != Y).mean()\n",
    "                train_loss.append((i,L))\n",
    "                train_erros.append((i,err_rate))\n",
    "                #if i % 100 == 0 :\n",
    "                #    print \"At minibatch %d, batch loss %f, batch error rate %f%%\" % (i, L, err_rate*100)\n",
    "                for P, V, G, N in zip(net.parameters, velocities, gradients, net.parameter_names):\n",
    "                    #\n",
    "                    # TODO: set a learning rate\n",
    "                    #\n",
    "                    # Hint, use the iteration counter i\n",
    "                    # alpha = alp[0] if i < alp_dropout else alp[1]\n",
    "                    alpha = alp[1] - (alp[1] - alp[0]) * (alp_dropout / (alp_dropout + i))\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: set the momentum constant\n",
    "                    #\n",
    "                    # TODO: implement velocity update in momentum\n",
    "                    #\n",
    "                    old_V = V\n",
    "                    if old_V == None:\n",
    "                        old_V = 0.\n",
    "                    V = epsilon * (old_V + G)\n",
    "                    \n",
    "                    if N=='W':\n",
    "                        #\n",
    "                        # TODO: implement the weight decay addition to gradient\n",
    "                        #\n",
    "                        G += P * lam\n",
    "                    \n",
    "                    \n",
    "                    #\n",
    "                    # TODO: set a more sensible learning rule here,\n",
    "                    # using your learning rate schedule and momentum\n",
    "                    #\n",
    "                    #!!!!! Need to modify the actual parameter here! \n",
    "                    P += -1 * alpha * G + old_V\n",
    "            # After an epoch compute validation error\n",
    "            val_error_rate = compute_error_rate(net, validation_stream)\n",
    "            if val_error_rate < best_valid_error_rate:\n",
    "                number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "                best_valid_error_rate = val_error_rate\n",
    "                best_params = deepcopy(net.parameters)\n",
    "                best_params_epoch = e\n",
    "                validation_errors.append((i,val_error_rate))\n",
    "            print \"| e %3d/%3d (%6d): err: %6.4f\" %(\n",
    "                e, number_of_epochs, i, val_error_rate),\n",
    "            if e % 3 == 0:\n",
    "                print \"\"\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print \"\\nSetting network parameters from after epoch %d\" %(best_params_epoch)\n",
    "        net.parameters = best_params\n",
    "        \n",
    "        subplot(2,1,1)\n",
    "        train_loss = np.array(train_loss)\n",
    "        semilogy(train_loss[:,0], train_loss[:,1], label='batch train loss')\n",
    "        legend()\n",
    "        \n",
    "        subplot(2,1,2)\n",
    "        train_erros = np.array(train_erros)\n",
    "        plot(train_erros[:,0], train_erros[:,1], label='batch train error rate')\n",
    "        validation_errors = np.array(validation_errors)\n",
    "        plot(validation_errors[:,0], validation_errors[:,1], label='validation error rate', color='r')\n",
    "        ylim(0,0.2)\n",
    "        legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| e   1/ 10 (   500): err: 0.0971 | e   2/ 10 (  1000): err: 0.0806 | e   3/ 10 (  1500): err: 0.0622 \n",
      "| e   4/ 10 (  2000): err: 0.0586 | e   5/ 10 (  2500): err: 0.0444 | e   6/ 10 (  3000): err: 0.0412 \n",
      "| e   7/ 12 (  3500): err: 0.0404 | e   8/ 13 (  4000): err: 0.0352 | e   9/ 15 (  4500): err: 0.0344 \n",
      "| e  10/ 17 (  5000): err: 0.0340 | e  11/ 18 (  5500): err: 0.0301 | e  12/ 20 (  6000): err: 0.0291 \n",
      "| e  13/ 21 (  6500): err: 0.0290 | e  14/ 21 (  7000): err: 0.0300 | e  15/ 25 (  7500): err: 0.0270 \n",
      "| e  16/ 26 (  8000): err: 0.0258 | e  17/ 26 (  8500): err: 0.0268 | e  18/ 26 (  9000): err: 0.0258 \n",
      "| e  19/ 26 (  9500): err: 0.0278 | e  20/ 33 ( 10000): err: 0.0251 | e  21/ 34 ( 10500): err: 0.0247 \n",
      "| e  22/ 34 ( 11000): err: 0.0250 | e  23/ 37 ( 11500): err: 0.0230 | e  24/ 37 ( 12000): err: 0.0232 \n",
      "| e  25/ 37 ( 12500): err: 0.0247 | e  26/ 37 ( 13000): err: 0.0271 | e  27/ 37 ( 13500): err: 0.0235 \n",
      "| e  28/ 37 ( 14000): err: 0.0241 | e  29/ 37 ( 14500): err: 0.0234 | e  30/ 37 ( 15000): err: 0.0278 \n",
      "| e  31/ 37 ( 15500): err: 0.0235 | e  32/ 37 ( 16000): err: 0.0247 | e  33/ 37 ( 16500): err: 0.0236 \n",
      "| e  34/ 55 ( 17000): err: 0.0225 | e  35/ 55 ( 17500): err: 0.0250 | e  36/ 58 ( 18000): err: 0.0210 \n",
      "| e  37/ 58 ( 18500): err: 0.0215 | e  38/ 58 ( 19000): err: 0.0235 | e  39/ 58 ( 19500): err: 0.0225 \n",
      "| e  40/ 58 ( 20000): err: 0.0229 | e  41/ 58 ( 20500): err: 0.0229 | e  42/ 58 ( 21000): err: 0.0215 \n",
      "| e  43/ 58 ( 21500): err: 0.0212 | e  44/ 58 ( 22000): err: 0.0225 | e  45/ 58 ( 22500): err: 0.0227 \n",
      "| e  46/ 58 ( 23000): err: 0.0219 | e  47/ 58 ( 23500): err: 0.0217 | e  48/ 58 ( 24000): err: 0.0214 \n",
      "| e  49/ 79 ( 24500): err: 0.0207 | e  50/ 79 ( 25000): err: 0.0220 | e  51/ 79 ( 25500): err: 0.0215 \n",
      "| e  52/ 79 ( 26000): err: 0.0217 | e  53/ 85 ( 26500): err: 0.0206 | e  54/ 85 ( 27000): err: 0.0220 \n",
      "| e  55/ 85 ( 27500): err: 0.0208 | e  56/ 85 ( 28000): err: 0.0216 | e  57/ 85 ( 28500): err: 0.0210 \n",
      "| e  58/ 85 ( 29000): err: 0.0209 | e  59/ 85 ( 29500): err: 0.0209 | e  60/ 85 ( 30000): err: 0.0208 \n",
      "| e  61/ 85 ( 30500): err: 0.0214 | e  62/ 85 ( 31000): err: 0.0217 | e  63/ 85 ( 31500): err: 0.0215 \n",
      "| e  64/ 85 ( 32000): err: 0.0209 | e  65/ 85 ( 32500): err: 0.0217 | e  66/ 85 ( 33000): err: 0.0217 \n",
      "| e  67/ 85 ( 33500): err: 0.0213 | e  68/ 85 ( 34000): err: 0.0211 | e  69/ 85 ( 34500): err: 0.0211 \n",
      "| e  70/ 85 ( 35000): err: 0.0215 | e  71/ 85 ( 35500): err: 0.0211 | e  72/116 ( 36000): err: 0.0205 \n",
      "| e  73/116 ( 36500): err: 0.0210 | e  74/116 ( 37000): err: 0.0216 | e  75/116 ( 37500): err: 0.0220 \n",
      "| e  76/116 ( 38000): err: 0.0215 | e  77/116 ( 38500): err: 0.0206 | e  78/116 ( 39000): err: 0.0208 \n",
      "| e  79/116 ( 39500): err: 0.0205 | e  80/116 ( 40000): err: 0.0207 | e  81/130 ( 40500): err: 0.0201 \n",
      "| e  82/130 ( 41000): err: 0.0205 | e  83/130 ( 41500): err: 0.0209 | e  84/130 ( 42000): err: 0.0213 \n",
      "| e  85/130 ( 42500): err: 0.0203 | e  86/130 ( 43000): err: 0.0204 | e  87/130 ( 43500): err: 0.0203 \n",
      "| e  88/130 ( 44000): err: 0.0202 | e  89/130 ( 44500): err: 0.0229 | e  90/145 ( 45000): err: 0.0196 \n",
      "| e  91/145 ( 45500): err: 0.0217 | e  92/145 ( 46000): err: 0.0206 | e  93/145 ( 46500): err: 0.0199 \n",
      "| e  94/145 ( 47000): err: 0.0213 | e  95/145 ( 47500): err: 0.0206 | e  96/145 ( 48000): err: 0.0209 \n",
      "| e  97/145 ( 48500): err: 0.0214 | e  98/145 ( 49000): err: 0.0207 | e  99/145 ( 49500): err: 0.0199 \n",
      "| e 100/145 ( 50000): err: 0.0201 | e 101/145 ( 50500): err: 0.0211 | e 102/145 ( 51000): err: 0.0219 \n",
      "| e 103/145 ( 51500): err: 0.0206 | e 104/145 ( 52000): err: 0.0210 | e 105/145 ( 52500): err: 0.0214 \n",
      "| e 106/145 ( 53000): err: 0.0209 | e 107/145 ( 53500): err: 0.0206 | e 108/145 ( 54000): err: 0.0216 \n",
      "| e 109/145 ( 54500): err: 0.0211 | e 110/145 ( 55000): err: 0.0205 | e 111/145 ( 55500): err: 0.0196 \n",
      "| e 112/145 ( 56000): err: 0.0200 | e 113/145 ( 56500): err: 0.0210 | e 114/145 ( 57000): err: 0.0201 \n",
      "| e 115/145 ( 57500): err: 0.0201 | e 116/145 ( 58000): err: 0.0200 | e 117/145 ( 58500): err: 0.0203 \n",
      "| e 118/145 ( 59000): err: 0.0201 | e 119/145 ( 59500): err: 0.0201 | e 120/145 ( 60000): err: 0.0208 \n",
      "| e 121/145 ( 60500): err: 0.0207 | e 122/145 ( 61000): err: 0.0203 | e 123/145 ( 61500): err: 0.0203 \n",
      "| e 124/145 ( 62000): err: 0.0204 | e 125/145 ( 62500): err: 0.0204 | e 126/145 ( 63000): err: 0.0203 \n",
      "| e 127/145 ( 63500): err: 0.0206 | e 128/145 ( 64000): err: 0.0207 | e 129/145 ( 64500): err: 0.0200 \n",
      "| e 130/145 ( 65000): err: 0.0207 | e 131/145 ( 65500): err: 0.0200 | e 132/145 ( 66000): err: 0.0208 \n",
      "| e 133/145 ( 66500): err: 0.0205 | e 134/145 ( 67000): err: 0.0206 | e 135/145 ( 67500): err: 0.0200 \n",
      "| e 136/145 ( 68000): err: 0.0206 | e 137/145 ( 68500): err: 0.0208 | e 138/145 ( 69000): err: 0.0210 \n",
      "| e 139/145 ( 69500): err: 0.0204 | e 140/145 ( 70000): err: 0.0204 | e 141/145 ( 70500): err: 0.0199 \n",
      "| e 142/145 ( 71000): err: 0.0197 | e 143/145 ( 71500): err: 0.0197 | e 144/145 ( 72000): err: 0.0208 \n",
      "| e 145/145 ( 72500): err: 0.0203 \n",
      "Test error rate: 0.019600\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# TODO: pick a network architecture here. The one below is just \n",
    "# softmax regression\n",
    "#\n",
    "\n",
    "#net = FeedForwardNet([\n",
    "#        AffineLayer(784,10),\n",
    "#        SoftMaxLayer()\n",
    "#        ])\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(784,100),\n",
    "        ReLULayer(),\n",
    "        AffineLayer(100,20),\n",
    "        TanhLayer(),\n",
    "        AffineLayer(20,10),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "\n",
    "SGD(net, mnist_train_stream, mnist_validation_stream, mnist_test_stream,\n",
    "    alp=(4e-2, 7e-3), alp_dropout=30000., pat=1.6, epsilon=1e-10, lam=1e-3)\n",
    "\n",
    "print \"\\nTest error rate: %f\" % (compute_error_rate(net, mnist_test_stream), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0196"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_error_rate(net, mnist_test_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
