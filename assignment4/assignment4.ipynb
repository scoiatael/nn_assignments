{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as sopt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from common.gradients import check_gradient, numerical_gradient, encode_params, decode_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IrisXFull is a (5, 150)-shaped matrix of float64\n",
      "IrisX2feats is a (3, 150)-shaped matrix of float64\n",
      "IrisY is a (1, 150)-shaped matrix of int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f79f927dbd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEQCAYAAABFtIg2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1EpIRERErf8DBweGKgS9fToAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f79f93f7810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Here we load the IRIS dataset.\n",
    "# We will create two datasets: one using all features, and one using just Petal Langth and Petal Width for visualizations\n",
    "#\n",
    "iris = datasets.load_iris()\n",
    "petal_length = iris.data[:,iris.feature_names.index('petal length (cm)')]\n",
    "petal_width = iris.data[:, iris.feature_names.index('petal width (cm)')]\n",
    "\n",
    "IrisXFull = np.vstack([np.ones_like(petal_length), iris.data.T])\n",
    "IrisX2feats = np.vstack([np.ones_like(petal_length), petal_length, petal_width])\n",
    "IrisY = iris.target.reshape(1,-1).astype(np.int64)\n",
    "\n",
    "print \"IrisXFull is a %s-shaped matrix of %s\" % (IrisXFull.shape, IrisXFull.dtype)\n",
    "print \"IrisX2feats is a %s-shaped matrix of %s\" % (IrisX2feats.shape, IrisX2feats.dtype)\n",
    "print \"IrisY is a %s-shaped matrix of %s\" % (IrisY.shape, IrisY.dtype)\n",
    "\n",
    "scatter(IrisX2feats[1,:], IrisX2feats[2,:], c=IrisY.ravel(), cmap='spring')\n",
    "xlabel('petal_length')\n",
    "ylabel('petal_width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Numerical and anaylical gradients differ: [ 0.          0.          0.          0.27911679 -0.03088321 -0.24821655\n -0.12133178  0.09466822  0.02666822  0.76491466 -0.16708534 -0.59775201\n  0.31822704 -0.04243962 -0.27577296] != [ -49.66666667  -49.66666667  -49.66666667 -248.63133333 -294.82133333\n -327.204      -169.76066667 -137.57666667 -147.70866667  -72.712      -211.58\n -275.74933333  -12.11866667  -65.858      -100.62466667]!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-4c3a7f1e2d72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[0miris_log_reg_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mTheta\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSoftMaxRegression_implementation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIrisXFull\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIrisY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m#Make sure that the gradient computation is OK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m \u001b[0mcheck_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miris_log_reg_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[0mcheck_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miris_log_reg_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i247926/Documents/nn_assignments/common/gradients.py\u001b[0m in \u001b[0;36mcheck_gradient\u001b[1;34m(f, X, delta, prec)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiffnorm\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mprec\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdiffnorm\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mgradnorm\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mprec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             raise Exception(\"Numerical and anaylical gradients differ: %s != %s!\" %\n\u001b[1;32m---> 30\u001b[1;33m                             (num_grad, fgrad))\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiffnorm\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mprec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Numerical and anaylical gradients differ: [ 0.          0.          0.          0.27911679 -0.03088321 -0.24821655\n -0.12133178  0.09466822  0.02666822  0.76491466 -0.16708534 -0.59775201\n  0.31822704 -0.04243962 -0.27577296] != [ -49.66666667  -49.66666667  -49.66666667 -248.63133333 -294.82133333\n -327.204      -169.76066667 -137.57666667 -147.70866667  -72.712      -211.58\n -275.74933333  -12.11866667  -65.858      -100.62466667]!"
     ]
    }
   ],
   "source": [
    "def SoftMaxRegression_implementation(ThetaFlat, X, Y=None, return_probabilities=False):\n",
    "    \"\"\"\n",
    "    Compute the outputs of a softmax classifier, or the loss and gradient\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ThetaFlat : \n",
    "        flat array of parameters containing (n_features*n_classes) entries\n",
    "    X :\n",
    "        array of features, shape n_features x n_smaples\n",
    "    Y :\n",
    "        optional array of desired targets of shape 1 x n_samples\n",
    "    return_probabilities : \n",
    "        if True, the probabilities are returned and Y is not used\n",
    "        if False, the los and gradient is computed on the X,Y pairs\n",
    "    \"\"\"\n",
    "    #X is num_features x num_samples\n",
    "    num_features, num_samples = X.shape\n",
    "\n",
    "    #Theta is num_features x num_classes\n",
    "    #we first reshape ThetaFlat into Theta\n",
    "    Theta = ThetaFlat.reshape(num_features, -1)\n",
    "    _, num_classes = Theta.shape\n",
    "\n",
    "    #Activation of softmax neurons\n",
    "    #A's shape should be num_classes x num_samples\n",
    "    #\n",
    "    A = Theta.T.dot(X)\n",
    "    #\n",
    "    \n",
    "    #Now compute the SoftMax function\n",
    "    #O will be a num_classes x num_samples matrix of probabilities assigned by our model  \n",
    "    #Stability optimization - for each subtract the maximum activation\n",
    "    O = A - A.max(0, keepdims=True)\n",
    "    #\n",
    "    # TODO - compute SoftMax as vector O. Take the exp and normalize, so all values of O\n",
    "    #        would sum to 1.0.\n",
    "    def norm(x):\n",
    "        e = exp(x)\n",
    "        ret = e/e.sum()\n",
    "        assert(abs(ret.sum() - 1.0) < 0.00001)\n",
    "        return ret\n",
    "    O = array([norm(o) for o in O])\n",
    "\n",
    "    if return_probabilities:\n",
    "        return O\n",
    "    \n",
    "    #The loss is the average per-sample nll (neg log likelihood)\n",
    "    #The nll is the sum of the logarithms of probabilities assigned to each class\n",
    "    Yr = Y.ravel()\n",
    "    correct_class_likelihoods = np.log(O[Yr, np.arange(num_samples)])\n",
    "    L = - 1.0/num_samples * np.sum(correct_class_likelihoods)\n",
    "\n",
    "    #For the softmax activation and cross-entropy loss, the derivative dNLL/dA has a simple form\n",
    "    #Please fill in its computation\n",
    "    #\n",
    "    # TODO\n",
    "    Yi_is_k = array([[1 if Yr[i] == k else 0 for k in range(num_classes)] for i in range(num_samples)]).T\n",
    "    assert(Yi_is_k.T.sum() == num_samples)\n",
    "    dLdA1 = array([[O[k,i] - (1 if Yr[i] == k else 0) for i in range(num_samples)] for k in range(num_classes)])\n",
    "    # assert((dLdA == (O - Yi_is_k)).all())\n",
    "    #\n",
    "    dLdA2 = array([[(1 if Yr[i] == k else 0)*(O[k,i]- 1.) for i in range(num_samples)] for k in range(num_classes)])\n",
    "    dLdA = dLdA2\n",
    "\n",
    "    #Now we compute the gradient of the loss with respect to Theta\n",
    "    dLdTheta = np.dot(X, dLdA.T)\n",
    "\n",
    "    #reshape gard into the shape of Theta, for fmin_l_bfsgb to work\n",
    "    return L, dLdTheta.reshape(ThetaFlat.shape)\n",
    "\n",
    "#Make a function for training on irises\n",
    "iris_log_reg_cost = lambda Theta: SoftMaxRegression_implementation(Theta, IrisXFull, IrisY, False)\n",
    "#Make sure that the gradient computation is OK\n",
    "check_gradient(iris_log_reg_cost, np.zeros((3*5,)))\n",
    "check_gradient(iris_log_reg_cost, np.random.rand(3*5)*2.0-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Numerical and anaylical gradients differ: [  1.19904087e-10   1.24344979e-10   1.33226763e-10   2.79116812e-01\n  -3.08831883e-02  -2.48216522e-01  -1.21331774e-01   9.46682264e-02\n   2.66682264e-02   7.64914666e-01  -1.67085334e-01  -5.97752001e-01\n   3.18227044e-01  -4.24396231e-02  -2.75772956e-01] != [ -49.66666667  -49.66666667  -49.66666667 -248.35222222 -294.85222222\n -327.45222222 -169.882      -137.482      -147.682       -71.94711111\n -211.74711111 -276.34711111  -11.80044444  -65.90044444 -100.90044444]!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-4141440cf63c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mThetaOpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmin_l_bfgs_b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miris_log_reg_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miprint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcheck_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miris_log_reg_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mThetaOpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/i247926/Documents/nn_assignments/common/gradients.py\u001b[0m in \u001b[0;36mcheck_gradient\u001b[1;34m(f, X, delta, prec)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiffnorm\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mprec\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdiffnorm\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mgradnorm\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mprec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             raise Exception(\"Numerical and anaylical gradients differ: %s != %s!\" %\n\u001b[1;32m---> 30\u001b[1;33m                             (num_grad, fgrad))\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiffnorm\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mprec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Numerical and anaylical gradients differ: [  1.19904087e-10   1.24344979e-10   1.33226763e-10   2.79116812e-01\n  -3.08831883e-02  -2.48216522e-01  -1.21331774e-01   9.46682264e-02\n   2.66682264e-02   7.64914666e-01  -1.67085334e-01  -5.97752001e-01\n   3.18227044e-01  -4.24396231e-02  -2.75772956e-01] != [ -49.66666667  -49.66666667  -49.66666667 -248.35222222 -294.85222222\n -327.45222222 -169.882      -137.482      -147.682       -71.94711111\n -211.74711111 -276.34711111  -11.80044444  -65.90044444 -100.90044444]!"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Call a solver\n",
    "#\n",
    "\n",
    "#iprint will cause the solver to print TO THE TERMINAL from which ipython notebook was started\n",
    "ThetaOpt = sopt.fmin_l_bfgs_b(iris_log_reg_cost, np.zeros((3*5,)), iprint=1)[0]\n",
    "\n",
    "check_gradient(iris_log_reg_cost, ThetaOpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Compute training errors\n",
    "#\n",
    "\n",
    "probabilities = SoftMaxRegression_implementation(ThetaOpt, IrisXFull, return_probabilities=True)\n",
    "predictions = np.argmax(probabilities,0)\n",
    "\n",
    "print \"Training accurracy: %f%%\" % ((predictions==IrisY.ravel()).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Now redo the training for two features\n",
    "#\n",
    "# TODO - again, use l_bfgs to find optimal theta, then compute probabilities and new predictions.\n",
    "#\n",
    "\n",
    "print \"Training accurracy: %f%%\" % ((predictions==IrisY.ravel()).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Now plot the decision boundary\n",
    "# \n",
    "\n",
    "petal_lengths, petal_widths = np.meshgrid(np.linspace(IrisX2feats[1,:].min(), IrisX2feats[1,:].max(), 100),\n",
    "                                          np.linspace(IrisX2feats[2,:].min(), IrisX2feats[2,:].max(), 100))\n",
    "\n",
    "IrisXGrid = np.vstack([np.ones(np.prod(petal_lengths.shape)), petal_lengths.ravel(), petal_widths.ravel()])\n",
    "predictions_Grid = SoftMaxRegression_implementation(Theta2class, IrisXGrid, return_probabilities=True).argmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contourf(petal_lengths, petal_widths, predictions_Grid.reshape(petal_lengths.shape), cmap='spring')\n",
    "scatter(IrisX2feats[1,:], IrisX2feats[2,:], c=IrisY.ravel(), cmap='spring')\n",
    "xlabel('petal_length')\n",
    "ylabel('petal_width')\n",
    "title('Decision boundary found by SoftMAx regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "\n",
    "Please note, that we **move the addition of the bias term** into the network implementation! Please note, that typically instead of changing the inputs we keep another vector of bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TwoLayerNet_implementation(ThetaFlat, ThetaShapes, X, Y=None, return_probabilities=False):\n",
    "    \"\"\"\n",
    "    Compute the outputs of a softmax classifier, or the loss and gradient\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ThetaFlat : \n",
    "        flat array of parameters\n",
    "    ThetaShapes :\n",
    "        list of shapes of weight and bias matrices\n",
    "    X :\n",
    "        array of features, shape n_features x n_smaples\n",
    "    Y :\n",
    "        optional array of desired targets of shape 1 x n_samples\n",
    "    return_probabilities : \n",
    "        if True, the probabilities are returned and Y is not used\n",
    "        if False, the los and gradient is computed on the X,Y pairs\n",
    "    \"\"\"\n",
    "    #X is num_features x num_samples\n",
    "    num_features, num_samples = X.shape\n",
    "\n",
    "    #Extract weight matrices\n",
    "    W1, W2 = decode_params(ThetaFlat, ThetaShapes)\n",
    "    \n",
    "    X_padded = np.vstack([np.ones((1, num_samples)), X])\n",
    "    \n",
    "    #Activation in first layer. Shape is num_hidden x num_samples\n",
    "    #\n",
    "    # TODO\n",
    "    # A1 = \n",
    "    #\n",
    "\n",
    "    #Apply the transfer function\n",
    "    #\n",
    "    # TODO\n",
    "    # H1 = \n",
    "    #\n",
    "        \n",
    "    #Pad with zeros\n",
    "    H1_padded = np.vstack([np.ones((1, num_samples)), H1])\n",
    "    \n",
    "    #Now apply the second linear transform\n",
    "    #\n",
    "    # TODO\n",
    "    # A2 = \n",
    "    #\n",
    "    \n",
    "    #Now compute the SoftMax function\n",
    "    #O will be a num_classes x num_samples matrix of probabilities assigned by our model  \n",
    "    #Stability optimization - for each subtract the maximum activation\n",
    "    O = A2 - A2.max(0, keepdims=True)\n",
    "    # \n",
    "    # TODO - compute SoftMax as vector O. Take the exp and normalize, so all values of O\n",
    "    #        would sum to 1.0.\n",
    "    # \n",
    "\n",
    "    if return_probabilities:\n",
    "        return O\n",
    "    \n",
    "    #The loss is the average per-sample nll (neg log likelihood)\n",
    "    #The nll is the sum of the logarithms of probabilities assigned to each class\n",
    "    correct_class_likelihoods = np.log(O[Y.ravel(), np.arange(num_samples)])\n",
    "    L = - 1.0/num_samples * np.sum(correct_class_likelihoods)\n",
    "\n",
    "    #For the softmax activation and cross-entropy loss, the derivative dNLL/dA has a simple form\n",
    "    #Please fill in its computation\n",
    "    #\n",
    "    # TODO\n",
    "    # dLdA2 = \n",
    "    #\n",
    "\n",
    "    dLdH1_padded = W2.dot(dLdA2)\n",
    "    dLdH1 = dLdH1_padded[1:,:] #ship the derivatives backpropagated to the added ones\n",
    "    \n",
    "    #\n",
    "    # TODO - compute the derivatives dLdW2 and dLdW1\n",
    "    # Hint - to compute dLdW1, start with dLdA1\n",
    "    #\n",
    "    \n",
    "    dLdThetaFlat, unused_shapes = encode_params([dLdW1, dLdW2])\n",
    "    \n",
    "    #reshape gard into the shape of Theta, for fmin_l_bfsgb to work\n",
    "    return L, dLdThetaFlat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Here we init the network for gradient testing on IRIS\n",
    "#\n",
    "# We will have 7 hidden neurons.\n",
    "# The first weight matrix will be 5 (4 features + bias) x 7 (hidden neurons)\n",
    "# The second weight matrix will be 8 (7 neurons + bias) x 3 (classes)\n",
    "#\n",
    "num_hidden = 7\n",
    "#\n",
    "# TODO\n",
    "# W1 = \n",
    "# W2 = \n",
    "#\n",
    "\n",
    "# Now flatten into an array\n",
    "Theta0, ThetaShape = encode_params([W1,W2])\n",
    "\n",
    "#Make a function for training on irises\n",
    "iris_net_cost = lambda Theta: TwoLayerNet_implementation(Theta, ThetaShape, iris.data.T, IrisY, False)\n",
    "#Make sure that the gradient computation is OK\n",
    "check_gradient(iris_net_cost, Theta0)\n",
    "check_gradient(iris_net_cost, np.zeros_like(Theta0))\n",
    "check_gradient(iris_net_cost, np.ones_like(Theta0)*0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# TODO - apply L-BFGS to minimize the loss and get optimal ThetaOpt.\n",
    "#\n",
    "predictions = TwoLayerNet_implementation(ThetaOpt, ThetaShape, iris.data.T, return_probabilities=True).argmax(0)\n",
    "print \"Training accurracy: %f%%\" % ((predictions==IrisY.ravel()).mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6\n",
    "\n",
    "Please note - we will act as if we had two classes. Thus we will use the softmax output of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XOR2X = np.array([[0,0],\n",
    "                  [0,1],\n",
    "                  [1,0],\n",
    "                  [1,1]]).T\n",
    "XOR2Y = np.array([[0,1,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#init the neurons\n",
    "num_hidden = 2\n",
    "W1 = (np.random.rand(3,num_hidden) - 0.5)\n",
    "W2 = (np.random.rand(num_hidden+1,2) -0.5)\n",
    "\n",
    "# Now flatten into an array\n",
    "Theta0, ThetaShape = encode_params([W1,W2])\n",
    "\n",
    "#\n",
    "# TODO - apply L-BFGS to minimize the loss and get optimal ThetaOpt.\n",
    "#\n",
    "\n",
    "TwoLayerNet_implementation(ThetaOpt, ThetaShape, XOR2X, return_probabilities=True).argmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# TODO - repeat the experiment for 3-dimensional XOR.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# (Bonus)\n",
    "# TODO - change network implementation code to return hidden activations.\n",
    "# Hint - locals() gives the dictionary of all objects in a functions's scope!\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IrisNormX = np.array(iris.data.T)\n",
    "#\n",
    "# TODO - normalize IrisNormX, so the vlaues would fall into [-1,1].\n",
    "#        Avoid looping constructs.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_hidden = 10\n",
    "\n",
    "W1 = (np.random.rand(5, num_hidden) - 0.5)*0.1\n",
    "W2 = (np.random.rand(num_hidden + 1, 3) - 0.5)*0.1\n",
    "\n",
    "# Now flatten into an array\n",
    "Theta0, ThetaShape = encode_params([W1, W2])\n",
    "\n",
    "#\n",
    "# TODO - cripple and train your neural network.\n",
    "#\n",
    "\n",
    "predictions = TwoLayerNet_implementation(ThetaOpt, ThetaShape, IrisNormX, return_probabilities=True).argmax(0)\n",
    "print \"Training accurracy: %f%%\" % ((predictions==IrisY.ravel()).mean()*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
