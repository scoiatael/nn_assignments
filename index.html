<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>NeuralNetworks - CIFAR10 by scoiatael</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>NeuralNetworks - CIFAR10</h1>
        <p>Code for Neural Networks 2015 class, w/ neural network for solving CIFAR10 project.</p>

        <p class="view"><a href="https://github.com/scoiatael/nn_assignments">View the Project on GitHub <small>scoiatael/nn_assignments</small></a></p>


        <ul>
          <li><a href="https://github.com/scoiatael/nn_assignments/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/scoiatael/nn_assignments/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/scoiatael/nn_assignments">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h2>
<a id="cifar10-solution" class="anchor" href="#cifar10-solution" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>CIFAR10 Solution</h2>

<p>This solution achieves about 75% accuracy on CIFAR10 dataset.</p>

<h3>
<a id="architecture" class="anchor" href="#architecture" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Architecture</h3>

<p>Architecture is based on, by many considered canonical, <a href="http://yann.lecun.com/exdb/lenet/">LeNet</a>. It consists of layers:</p>

<ul>
<li>NL - linear + ReLU nonlinearity,</li>
<li>C2 - 2d convolution,</li>
<li>M2 - 2d max pooling,</li>
<li>DB - dropout of 70%,</li>
<li>DS - dropout of 30%,</li>
<li>SM - softmax layer.
They are structured in following way:</li>
</ul>

<p><img src="https://cloud.githubusercontent.com/assets/2892794/12592029/ee77ee94-c46a-11e5-9324-9b60988f9648.png" alt="C2 -&gt; M2 -&gt; C2 -&gt; M2 -&gt; C2 -&gt; DS -&gt; NL -&gt; DB -&gt; NL -&gt; SM."></p>

<p>This way convolution layers should act as feature extractors, nonlinear layers should act as multilayer perceptron, and softmax layer should produce desired results. Dropout layers were added as a means of reducing chance of overfitting.</p>

<h3>
<a id="training" class="anchor" href="#training" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Training</h3>

<p>Network was trained using Nesterov momentum (improved version of Stochastic Gradient Descent).</p>

<p>Network was trained on examples from normal and inverted (features scaled by -1) train stream. Each training pass consisted of doing 5 full passes over inverted train set, then 10 over normal. Each full pass updated network.</p>

<p>Motivation of using inverted features was to force net to look at shape, not strictly colour of things. To make up for extra noise generated by these examples, they were assigned much lower learning rate (0.1 of the original) and had less passes.To fully assess that approach, more data should be tested and generalized, although here it showed some promise: net trained much faster and with better overall results in this manner. But this may be simply because those examples generated extra noise.</p>

<p>Majority (4/5) of train examples were used for training, and rest for validation.</p>

<p>Whole training took 25 training passes (which means that network was fed train set 500 times). With each train set pass taking between as few as 5 seconds (thanks to Theano CUDA/GPU support) up to 20 seconds, this was much shorter then expected time for CIFAR10 network.</p>

<h3>
<a id="tools" class="anchor" href="#tools" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Tools</h3>

<p>Network was written with <a href="http://lasagne.readthedocs.org/en/latest/">Lasagne</a>, a simple Python library for building and training neural networks in Theano. It provides user with ready-to-use variants of most popular layers and training functions. For the purposes of this network, it had everything in place.</p>

<h3>
<a id="hyper-parameters" class="anchor" href="#hyper-parameters" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Hyper parameters</h3>

<p>Hyper parameters were chosen by trial-and-error, with running multiple versions of network and choosing those that performed best. Initial values were taken from example networks for MNIST from lasagne repository.</p>

<ul>
<li><p><strong>Momentum</strong> equal to <code>0.2</code>. With initial value <code>0.9</code> network was initially learning very quickly, but also stopped learning pretty quickly. Decreasing momentum further slowed down initial progress, but increased overall accuracy.</p></li>
<li><p><strong>Learning rate</strong> equal to <code>0.01*0.1**(iteration_numer/100.0)</code>. While constant learning rate is often suggested, here it was found that decreasing learning rate with time offered better results.</p></li>
<li><p>Convolution layers used <code>32</code> <strong>learnable convolutional filters</strong> and <code>5x5</code> <strong>filter size</strong>. It was found that increasing filter size actually decreased network accuracy, while increasing filter size made no difference for final accuracy but increased learning time.</p></li>
<li><p>Max pooling layers used <code>2x2</code> <strong>pool size</strong>. With 32x32 images, increasing that parameter was impossible.</p></li>
<li><p>Dropout layers used <strong>dropout probability</strong> equal to <code>0.7</code> (for deeper layers) and <code>0.3</code> (for closer layers).</p></li>
</ul>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/scoiatael">scoiatael</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
