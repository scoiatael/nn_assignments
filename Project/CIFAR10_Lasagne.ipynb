{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 25 days\n",
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 25 days\n",
      "Using gpu device 0: GeForce GTX 780 (CNMeM is disabled)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GpuElemwise{exp,no_inplace}(<CudaNdarrayType(float32, vector)>), HostFromGpu(GpuElemwise{exp,no_inplace}.0)]\n",
      "Looping 1000 times took 0.371182 seconds\n",
      "Result is [ 1.23178029  1.61879349  1.52278066 ...,  2.20771813  2.29967761\n",
      "  1.62323296]\n",
      "Used the gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from theano import function, config, shared, sandbox\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], T.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in xrange(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIFAR10((\"train\",)).num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OFFSET = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CIFAR10.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}) )\n",
    "#    (Flatten, [], {'which_sources': 'targets'}))\n",
    "#    (Mapping, [lambda batch: (b.ravel() for b in batch)], {'which_sources': 'targets'}) )\n",
    "\n",
    "mnist_train = CIFAR10((\"train\",), subset=slice(None,OFFSET))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 100))\n",
    "                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each training batch consits of a tuple containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print(\"The streams return batches containing %s\" % (mnist_train_stream.sources,))\n",
    "\n",
    "print(\"Each training batch consits of a tuple containing:\")\n",
    "for element in next(mnist_train_stream.get_epoch_iterator()):\n",
    "    print(\" - an array of size %s containing %s\" % (element.shape, element.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist_validation = CIFAR10((\"train\",), subset=slice(OFFSET, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 250))\n",
    "mnist_test = CIFAR10((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (250, 3, 32, 32) containing float32\n",
      " - an array of size (250, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation/test batches consits of tuples containing:\")\n",
    "for element in next(mnist_test_stream.get_epoch_iterator()):\n",
    "    print(\" - an array of size %s containing %s\" % (element.shape, element.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_in = lasagne.layers.InputLayer(shape=(None, 3, 32, 32),\n",
    "                                     input_var=input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'nonlinear_units_big' : 2000,\n",
    "    'nonlinear_units_small' : 500,\n",
    "    'dropout_small' : .1,\n",
    "    # (.2 -> .1)\n",
    "    'dropout_big' : .2,\n",
    "    # (.4 -> .2)\n",
    "    'learning_rate' : .0001, \n",
    "    # (.01 -> .005)\n",
    "    # (.001 -> 0.0001)\n",
    "    'momentum' : .1,\n",
    "    # -> (.6 -> .4) .9 seems too harsh (net overshoots a lot)\n",
    "    # -> (.2 -> .1)\n",
    "    'num_filters' : 32,\n",
    "    'filter_size' : (5,5), # over (5,5) decrease net capabilities\n",
    "    'pool_size' : (2,2) # keep this (2,2) -> how much we need to scale before MLP\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network_combinator(in_layer, layers):\n",
    "    for layer_def in layers:\n",
    "        in_layer = layer_def[0](in_layer, **layer_def[1])\n",
    "    return in_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_layer(l_in):\n",
    "    return network_combinator(l_in,[\n",
    "            ( lasagne.layers.DenseLayer, {\n",
    "                    'num_units'  : 10,\n",
    "                    'nonlinearity' : lasagne.nonlinearities.softmax })\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mlp(l_in):\n",
    "    dropout = lambda p:  ( lasagne.layers.DropoutLayer, { 'p' : p } )\n",
    "    dense_nonlinear = lambda u:  ( lasagne.layers.DenseLayer, {\n",
    "                    'nonlinearity' : lasagne.nonlinearities.rectify,\n",
    "                    'num_units' : u,\n",
    "                })\n",
    "    nlub = parameters['nonlinear_units_big']\n",
    "    ds = parameters['dropout_small']\n",
    "    db = parameters['dropout_big']\n",
    "    return network_combinator(l_in,[\n",
    "            dropout(ds),\n",
    "            dense_nonlinear(nlub),\n",
    "            dropout(db),\n",
    "            dense_nonlinear(nlub),\n",
    "            dropout(db),\n",
    "            dense_nonlinear(nlub),\n",
    "            dropout(db),\n",
    "            dense_nonlinear(nlub)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cnn(l_in):\n",
    "    conv2d = lambda f, s: ( lasagne.layers.Conv2DLayer, {\n",
    "            'num_filters' : f,\n",
    "            'filter_size' : s,\n",
    "            'nonlinearity' : lasagne.nonlinearities.rectify\n",
    "            })\n",
    "    \n",
    "    \n",
    "    maxpool2d = lambda s: ( lasagne.layers.MaxPool2DLayer, {\n",
    "            'pool_size' : s\n",
    "        })\n",
    "    \n",
    "    nfilt = parameters['num_filters']\n",
    "    filts = parameters['filter_size']\n",
    "    pools = parameters['pool_size']\n",
    "    \n",
    "    return network_combinator(l_in, [\n",
    "            conv2d(nfilt, filts),\n",
    "            maxpool2d(pools),\n",
    "            conv2d(nfilt, filts),\n",
    "            maxpool2d(pools),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network = l_in\n",
    "network = cnn(network)\n",
    "network = mlp(network)\n",
    "network = final_layer(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=parameters['learning_rate'], momentum=parameters['momentum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_stream, validation_stream, num_epochs, f=sys.stdout):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for inputs, targets in train_stream.get_epoch_iterator():\n",
    "            train_err += train_fn(inputs, targets.ravel())\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for inputs, targets in validation_stream.get_epoch_iterator():\n",
    "            err, acc = val_fn(inputs, targets.ravel())\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time),\n",
    "            file=f)\n",
    "#        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches)\n",
    "#              , file=f)\n",
    "#        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches)\n",
    "#              , file=f)\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100)\n",
    "              , file=f)\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1 took 8.209s\n",
      "  validation accuracy:\t\t9.49 %\n"
     ]
    }
   ],
   "source": [
    "train(mnist_train_stream, mnist_validation_stream, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('workfile', 'w', 0)\n",
    "try:\n",
    "    train(mnist_train_stream, mnist_validation_stream, 1000, f = f)\n",
    "finally:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train accuracy:\t\t76.47 %\n"
     ]
    }
   ],
   "source": [
    "val_acc = 0\n",
    "batches = 0\n",
    "for inputs, targets in mnist_test_stream.get_epoch_iterator():\n",
    "    _, acc = val_fn(inputs, targets.ravel())\n",
    "    val_acc += acc\n",
    "    batches += 1\n",
    "print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "    val_acc / batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('parameters', 'w')\n",
    "try:\n",
    "    pickle.dump(network, f)\n",
    "finally:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.save('nparameters', network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
